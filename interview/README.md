## **Tell Me About Yourself (DevOps) â€“ 3 Years Experience**

### **Introduction:**
Hi, my name is [Your Name], and I have three years of experience in DevOps. Throughout my career, I have worked on automating infrastructure, CI/CD pipelines, and cloud deployments to improve software delivery efficiency.

### **Technical Experience:**
- **Infrastructure as Code (IaC):** Hands-on experience with Terraform and Ansible to automate infrastructure provisioning.
- **Cloud Platforms:** Worked with AWS, managing EC2 instances, S3, IAM roles, and VPC configurations.
- **CI/CD Pipelines:** Set up and managed CI/CD pipelines using Jenkins/GitHub Actions/GitLab CI to automate code deployment.
- **Containerization:** Experience with Docker and Kubernetes for containerized application deployment and orchestration.
- **Monitoring & Logging:** Worked with Prometheus, Grafana, and ELK stack for monitoring and logging.
- **Version Control:** Use Git and GitHub/GitLab for source code management and collaboration.

### **Work Approach & Problem-Solving:**
I focus on automation and scalability, ensuring infrastructure is reliable and easy to manage. I enjoy solving complex deployment issues and optimizing DevOps workflows to enhance system performance and security.

### **Closing:**
I am always eager to learn new technologies and improve DevOps processes. I am excited about the opportunity to contribute my skills and grow in this role.

#######################################################################################################
#######################################################################################################

# Professional Summary

- Having 3 years of experience as an DevOps Engineer.
- Extensive experience in continuous integration, continuous deployment, continuous delivery, supporting build pipelines, release management, configuration management, infrastructure management, and cloud computing.
- Expertise in Jenkins, Ansible, Terraform, Kubernetes & AWS Cloud.

# Technical Skills

**Versioning Tool:** Git  
**CM Tools:** Ansible  
**Infra Provisioning Tool:** Terraform  
**CI/CD Tool:** Jenkins  
**Build Tool:** Maven  
**Static Code Analysis:** SonarQube  
**Artifactory:** Nexus Repository  
**Container Tool:** Docker  
**Orchestration Tool:** Kubernetes  
**Operating Systems:** Linux, Windows  
**Monitoring Tools:** Prometheus and Grafana  
**Scripting:** Python, Bash  
**Web/App Servers:** Nginx  
**Cloud Services (AWS):** EC2, ECR, EKS, S3, Auto-Scaling, ELB, WAF, Elastic Beanstalk, RDS, VPC, CloudWatch, CloudFormation, IAM, SNS, Lambda, SQS, and API Gateway

# Experience

# Client: HSBC

## Project: Implementation & Supporting

**Period:** March 2016 to 2019  
**Role:** DevOps Engineer

## DESCRIPTION:
The Design TO Cost (DTC) is a web-based application for tracking and managing product cost with cost-related data for a new product development program. Besides tracking the product cost data, the tool also provides for workflow and facilitates collaboration amongst team members working on a program. This manual introduces the features of the tool together with instructions on how to use the tool for managing product cost data. This manual organizes such features into sections that define the various user roles.

## Responsibilities:
- Highly motivated and committed AWS Engineer experienced in Automating, Configuring, and deploying instances on AWS and also familiar with EC2, CloudWatch, Elastic IPs, and managing security groups on AWS.
- Deployed the Java application into web application servers like Apache Tomcat.
- Launching Amazon EC2 Cloud Instances using Amazon Web Services (Linux/Ubuntu) and configuring launched instances with respect to specific applications.
- Create Chef automation tools and builds, and do an overall process improvement to any manual processes.
- Experienced in build and deployment of Java applications onto different environments such as QA, UAT, and Production.
- Virtualized the servers using Docker for test environments and dev-environments needs. Also performed configuration automation using Docker containers.
- Working on AWS Auto Scaling for providing high availability of applications and EC2 instances based on the load of applications by using CloudWatch in AWS.
- EBS Volumes management and working on creating snapshots for backups manually and using scripts.
- Launching Amazon EC2 Cloud Instances using Amazon Web Services (Linux/Ubuntu/RHEL) and configuring launched instances with respect to specific applications.




#######################################################################################################
#######################################################################################################

## Environment
Linux, Jenkins, GIT, Docker Images, AWS, Ansible, Terraform, Kubernetes (EKS), ECR, ASG ,Elk.

## Responsibilities

- Expertise in making code DRY (Don't Repeat Yourself) in Jenkins, Terraform, Ansible, and Kubernetes.
- Expertise in CI/CD pipelines using Jenkins and multi-environment provisioning using Terraform.
- Created CI/CD pipelines by integrating Git, Jenkins, Ansible, and Terraform. Automated deployment using Ansible as a configuration management tool.
- Experience in DevOps Engineering, automating, building, and deploying code within different environments (Dev, QA, Pre-prod, and Prod). Coordinated with the dev team to fix build-related issues.
- Experience with continuous integration tools like Jenkins. Created Continuous Integration and Continuous Deployment pipelines. Developed Terraform modules and used them in multiple stages of Jenkins CD pipeline.
- Worked with configuration management, using Terraform Post-Provisioners to deploy applications on EC2 instances.
- Migrated mutable infrastructure to cloud-managed Kubernetes clusters (EKS) using aws DevOps.
- Created Traffic, Latency, Errors, and Saturation dashboards for the SRE team using Elk.
- Created EC2 analytics dashboards for cost savings inElk.

### Automation Tasks (Multiple AWS Accounts)

- Listed all drifted CloudFormation stacks using a Python script that saves results to a CSV file.
- Identified unutilized AWS resources using a Python script that saves results to a CSV file.
- Updated deprecated Python runtime from 2.7 to 3.9 for Lambda functions.
- Used Python to tag AWS resources.
- Listed CloudWatch datasets to a CSV file for CPU, Network In, and Network Out using Python (average CPU for 10 days, 20 days, 30 days, etc.).

